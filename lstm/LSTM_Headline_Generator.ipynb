{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Headlines in English Using LSTM"
      ],
      "metadata": {
        "id": "SN4SwHk9KYEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s fast-paced digital world, the ability to create compelling and relevant headlines is crucial for capturing audience attention and driving engagement. Headlines serve as the first impression of content, influencing readers' decisions to explore articles further. Given the growing volume of content and the demand for timely information, automating the headline creation process presents a significant opportunity.\n",
        "\n",
        "This project addresses this need by leveraging advanced machine learning techniques to automate the generation of headlines. The core of this approach is based on Long **Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN)** renowned for its ability to handle sequences and long-term dependencies in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "b1YXGqeYKYUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why LSTM for Headline Generation?"
      ],
      "metadata": {
        "id": "Z7CW3CElKYWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional algorithms for text generation often struggle with maintaining coherence over longer sequences, leading to headlines that may lack relevance or readability. LSTMs, with their specialized architecture, are designed to remember and use contextual information from earlier parts of the sequence. This makes them particularly effective for generating text that is not only grammatically correct but also contextually appropriate."
      ],
      "metadata": {
        "id": "GJEQlJbpLX7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals"
      ],
      "metadata": {
        "id": "DGaPlp7PL4lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to develop an **LSTM-based model** that can generate high-quality, engaging headlines in English. By training the model on a diverse dataset of existing headlines, we aim to produce headlines that are not only accurate but also creative and relevant. This model has the potential to assist content creators, journalists, and marketers by providing them with a tool to quickly generate impactful headlines, thereby enhancing productivity and content engagement."
      ],
      "metadata": {
        "id": "aIBJNtDGL6jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Reading the dataset"
      ],
      "metadata": {
        "id": "4HhcL-SmJ5W8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Jtjo41iWCthm"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/dataset.txt\", encoding=\"latin-1\") as f:\n",
        "    dataset = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi6EezaKJxD5",
        "outputId": "f75e7f0d-5da9-44ac-ecbe-dd4b4cc73b70"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['New energy law promises to revolutionize the electric sector',\n",
              " 'Climate change continues to be a global threat',\n",
              " 'Investors seek opportunities in renewable energy',\n",
              " 'Demand for electric vehicles increases',\n",
              " 'COVID-19 vaccines: When will we all be protected?',\n",
              " 'The debate over vaccines continues to divide opinions',\n",
              " 'Health experts analyze the effectiveness of vaccines',\n",
              " 'Mass vaccination against coronavirus underway',\n",
              " 'Cryptocurrency market soars to new heights',\n",
              " 'Is Bitcoin the currency of the future?']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation"
      ],
      "metadata": {
        "id": "Tpsjv0-nMdtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Data Cleaning"
      ],
      "metadata": {
        "id": "2xk0OOFQMd2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import unicodedata\n",
        "\n",
        "def clean_and_normalize_text(txt):\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    txt = \"\".join(c for c in txt if c not in string.punctuation).lower()\n",
        "    # Normalize unicode characters and encode to ASCII\n",
        "    txt = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return txt"
      ],
      "metadata": {
        "id": "TBT27anGMeHW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [clean_and_normalize_text(headline) for headline in dataset]"
      ],
      "metadata": {
        "id": "aLxrlsArN_JV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag3oEpUGOLFD",
        "outputId": "aca13021-7d9c-43df-a5c1-2079030f690e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['new energy law promises to revolutionize the electric sector',\n",
              " 'climate change continues to be a global threat',\n",
              " 'investors seek opportunities in renewable energy',\n",
              " 'demand for electric vehicles increases',\n",
              " 'covid19 vaccines when will we all be protected',\n",
              " 'the debate over vaccines continues to divide opinions',\n",
              " 'health experts analyze the effectiveness of vaccines',\n",
              " 'mass vaccination against coronavirus underway',\n",
              " 'cryptocurrency market soars to new heights',\n",
              " 'is bitcoin the currency of the future']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `clean_and_normalize_text` is designed to prepare text data for further processing by cleaning and standardizing it. This is a crucial step in text analysis and natural language processing. The function accomplishes the following:\n",
        "\n",
        "**Remove Unwanted Characters:**\n",
        "\n",
        "* **Objective:** Eliminate punctuation marks from the text.\n",
        "* **Why:** Punctuation can interfere with text analysis tasks such as text classification or tokenization. Removing it **helps in focusing on the core content of the text.**\n",
        "\n",
        "**Standardize Text:**\n",
        "\n",
        "* **Objective:** Normalize the text by converting it to lowercase and removing any special or accented characters.\n",
        "* **Why:** Converting the text to lowercase ensures uniformity, as \"Hello\" and \"hello\" would be treated as the same word. Normalizing accents and special characters helps in handling text from different sources and languages consistently, making it easier to analyze and compare.\n",
        "\n",
        "`clean_and_normalize_text` transforms raw text into a cleaner, more uniform format. This preprocessing step is essential for effective text analysis, improving the accuracy and reliability of subsequent processing tasks such as machine learning model training or text-based querying."
      ],
      "metadata": {
        "id": "ByDmBRcIM9QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Data Tokenization"
      ],
      "metadata": {
        "id": "rRPyBwKAOU8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "aKwmnUFSOZz5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_token_sequences(tokenizer, dataset):\n",
        "    # Build the tokenizer\n",
        "    tokenizer.fit_on_texts(dataset)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Tokenize the text in the dataset\n",
        "    dataset_tokens = []\n",
        "    for text in dataset:\n",
        "        text_tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        # Generate n-grams from the tokenized text\n",
        "        for i in range(1, len(text_tokens)):\n",
        "            n_gram = text_tokens[:i+1]\n",
        "            dataset_tokens.append(n_gram)\n",
        "\n",
        "    return dataset_tokens, total_words\n"
      ],
      "metadata": {
        "id": "pAQjPyKsO_1d"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_tokens, total_words = generate_token_sequences(tokenizer, dataset)"
      ],
      "metadata": {
        "id": "LZqeNrTIPkAG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `generate_token_sequences` is used to preprocess a text dataset by tokenizing the text and generating n-grams. It first builds a tokenizer based on the dataset, then converts each text into sequences of integers, and finally creates and collects various n-grams (sub-sequences) from the tokenized text. This preprocessing step is essential for transforming raw text data into a structured format **suitable for training machine learning models or performing further text analysis.**"
      ],
      "metadata": {
        "id": "BY7nrcsPPDrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this section is to prepare the target labels for each sequence in the dataset. In the context of sequence-based models, such as those used for text generation, the labels represent the expected output for each input sequence."
      ],
      "metadata": {
        "id": "z45VQwueQLRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "dwsXLWRGS45k"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Understanding Sequences and Labels:**\n",
        "\n",
        "* **Sequences:** The input data consists of sequences of tokens (e.g., words or characters) where each sequence is used to predict the next token.\n",
        "* **Labels:** **The label for each sequence is the token that immediately follows the sequence**. For example, if the sequence is \"The cat is\", the label would be \"on\" if the full text was \"The cat is on the mat\".\n",
        "\n",
        "**Generating Input Sequences and Labels:**\n",
        "\n",
        "* **Input Sequences:** Each sequence of tokens is used to predict the next token. To prepare this, each sequence needs to be split so that the model can learn from the sequence to predict the next token.\n",
        "* **Labels:** For each sequence, the corresponding label is the next token in the sequence. The model will learn to map the input sequence to this label.\n",
        "Implementation Details:\n",
        "\n",
        "**Here’s a step-by-step breakdown of how this is generally implemented:**\n",
        "\n",
        "**Determine Maximum Sequence Length:**\n",
        "\n",
        "Calculate the length of the longest sequence in the dataset to ensure that all sequences are padded to the same length.\n",
        "* **Apply Padding:**\n",
        "Use padding to standardize the length of all sequences. Padding involves adding zeros (or another value) to the beginning or end of sequences to make them all the same length.\n",
        "* **Split Sequences:**\n",
        "\n",
        "Separate each padded sequence into two parts:\n",
        "* **Input Part:** All tokens except the last one.\n",
        "* **Label Part:** The last token in the sequence.\n",
        "\n",
        "**Convert Labels to One-Hot Encoding:**\n",
        "\n",
        "If the task is classification, convert the labels into a one-hot encoded format where each label is represented as a binary vector indicating the presence of a token in the vocabulary."
      ],
      "metadata": {
        "id": "3hr_wBhNQLhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.utils as ku\n",
        "\n",
        "def prepare_sequences_and_labels(dataset_tokens, total_words):\n",
        "    # Determine the length of the longest sequence in the dataset\n",
        "    max_sequence_len = max([len(text) for text in dataset_tokens])\n",
        "\n",
        "    # Apply padding to all sequences to ensure they are of the same length\n",
        "    dataset_tokens = np.array(pad_sequences(dataset_tokens, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    # Generate input features and target labels\n",
        "    X_train, y_train = dataset_tokens[:, :-1], dataset_tokens[:, -1]\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    y_train = ku.to_categorical(y_train, num_classes=total_words)\n",
        "\n",
        "    return X_train, y_train, max_sequence_len\n"
      ],
      "metadata": {
        "id": "pd0-Avc1QLpj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, max_sequence_len = prepare_sequences_and_labels(dataset_tokens, total_words)"
      ],
      "metadata": {
        "id": "fEN0H5l9R_nQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTduP4TCSFOl",
        "outputId": "c417fe1c-2483-4f98-cf0c-8e2e80a83716"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `prepare_sequences_and_labels` is used to preprocess token sequences for model training. It pads sequences to ensure uniform length, generates input features and target labels from the sequences, and converts labels into a one-hot encoded format. This preprocessing step is essential for preparing data in a format suitable for training machine learning models, particularly in tasks such as sequence prediction."
      ],
      "metadata": {
        "id": "3r2ZtWFGQUw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Model Architecture and Design for Sequence Prediction"
      ],
      "metadata": {
        "id": "eTTQg8B5Bviz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, BatchNormalization"
      ],
      "metadata": {
        "id": "Oy3rFilSBvsj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Input Embedding Layer\n",
        "    model.add(Embedding(input_dim=total_words, output_dim=50))\n",
        "\n",
        "    # Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(128, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Add Hidden Layer 2 - LSTM Layer\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Add Batch Normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "IN1wZvAaBvwQ",
        "outputId": "f6af2a7b-b59e-40d5-ee6c-f88f52f897d3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model we’ve designed is aimed at handling sequence prediction tasks, like generating text. It utilizes several advanced techniques to make the most out of the sequence data.\n",
        "\n",
        "Starting with the **Embedding Layer**, we convert our tokens (words or characters) into **dense vector representations**. This layer maps each unique token in our vocabulary to a high-dimensional vector space. We’ve chosen an embedding dimension of 50, which means each token is represented by a 50-dimensional vector. This allows the model to capture and learn subtle semantic meanings and relationships between different tokens.\n",
        "\n",
        "Next, **we have two LSTM (Long Short-Term Memory) layers**. LSTMs are a type of recurrent neural network designed to handle sequences of data, making them ideal for tasks where understanding context over time is crucial, such as predicting the next word in a sentence.\n",
        "\n",
        "The first LSTM layer has 128 units and is set to return sequences. This means that instead of outputting just the final state of the sequence, it outputs the state at each time step. This is important because we want to pass the sequence of outputs to the next LSTM layer. To prevent overfitting, which can occur when the model learns the training data too well but performs poorly on unseen data, we apply a dropout rate of 20%. Dropout randomly disables a fraction of neurons during training, which forces the model to learn more robust features.\n",
        "\n",
        "Following the first LSTM layer, we add a second LSTM layer with 64 units. This layer processes the sequences output by the first LSTM layer and helps in refining the learned patterns. **Again, we apply a 20% dropout rate to ensure the model generalizes well.**\n",
        "\n",
        "To further stabilize and enhance training, we include a Batch Normalization layer. Batch normalization normalizes the output of the previous layer by adjusting and scaling the activations. This technique helps in accelerating training and achieving better model performance by reducing internal covariate shift.\n",
        "\n",
        "Finally, we arrive at the **Dense Output Layer**. This layer has as many units as there are unique tokens in our vocabulary, each with a softmax activation function. The softmax function converts the raw output into probabilities, indicating how likely each token is to follow the given sequence. **This output layer enables the model to make predictions about what the next token should be based on the learned sequences.**\n",
        "\n",
        "When we compile the model, we use c**ategorical crossentropy as the loss function, which is suitable for multi-class classification problems like this**. **The Adam optimizer is chosen for its efficiency and ability to handle sparse gradients**. We also track accuracy as a metric to monitor how well the model is performing during training.\n",
        "\n",
        "Overall, this architecture is designed to capture the temporal dependencies in sequences through LSTM layers, while techniques like dropout and batch normalization enhance its ability to generalize and perform effectively on unseen data. This setup aims to be robust and effective for generating meaningful sequences or text predictions."
      ],
      "metadata": {
        "id": "q_p1PUt9Cdis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUO_nKHoBvzK",
        "outputId": "4be47aea-3f22-4d03-b377-a095ed80e783"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.0453 - loss: 6.8834\n",
            "Epoch 2/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.1442 - loss: 5.3420\n",
            "Epoch 3/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.1830 - loss: 4.8335\n",
            "Epoch 4/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.2305 - loss: 4.3390\n",
            "Epoch 5/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.2684 - loss: 3.9763\n",
            "Epoch 6/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.3011 - loss: 3.6732\n",
            "Epoch 7/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.3587 - loss: 3.3067\n",
            "Epoch 8/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.3859 - loss: 3.0602\n",
            "Epoch 9/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.4221 - loss: 2.7607\n",
            "Epoch 10/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.4480 - loss: 2.5522\n",
            "Epoch 11/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.4849 - loss: 2.3286\n",
            "Epoch 12/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.5122 - loss: 2.1974\n",
            "Epoch 13/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5388 - loss: 2.0427\n",
            "Epoch 14/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5521 - loss: 1.9060\n",
            "Epoch 15/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5902 - loss: 1.7649\n",
            "Epoch 16/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6064 - loss: 1.6554\n",
            "Epoch 17/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6080 - loss: 1.5894\n",
            "Epoch 18/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6281 - loss: 1.5261\n",
            "Epoch 19/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6458 - loss: 1.4249\n",
            "Epoch 20/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6527 - loss: 1.3741\n",
            "Epoch 21/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6718 - loss: 1.2909\n",
            "Epoch 22/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6597 - loss: 1.3146\n",
            "Epoch 23/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6821 - loss: 1.2089\n",
            "Epoch 24/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6798 - loss: 1.2591\n",
            "Epoch 25/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6900 - loss: 1.1771\n",
            "Epoch 26/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7009 - loss: 1.1549\n",
            "Epoch 27/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7019 - loss: 1.1391\n",
            "Epoch 28/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7105 - loss: 1.0873\n",
            "Epoch 29/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7108 - loss: 1.0966\n",
            "Epoch 30/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7239 - loss: 1.0462\n",
            "Epoch 31/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7232 - loss: 1.0400\n",
            "Epoch 32/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7383 - loss: 0.9966\n",
            "Epoch 33/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7273 - loss: 1.0001\n",
            "Epoch 34/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7414 - loss: 0.9716\n",
            "Epoch 35/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7403 - loss: 0.9807\n",
            "Epoch 36/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7395 - loss: 0.9579\n",
            "Epoch 37/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7442 - loss: 0.9578\n",
            "Epoch 38/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7493 - loss: 0.9292\n",
            "Epoch 39/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7525 - loss: 0.8934\n",
            "Epoch 40/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7554 - loss: 0.9071\n",
            "Epoch 41/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7555 - loss: 0.8904\n",
            "Epoch 42/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7520 - loss: 0.8917\n",
            "Epoch 43/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7508 - loss: 0.8944\n",
            "Epoch 44/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7594 - loss: 0.8600\n",
            "Epoch 45/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7648 - loss: 0.8707\n",
            "Epoch 46/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7618 - loss: 0.8464\n",
            "Epoch 47/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7764 - loss: 0.8251\n",
            "Epoch 48/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7727 - loss: 0.8281\n",
            "Epoch 49/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7681 - loss: 0.8302\n",
            "Epoch 50/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7656 - loss: 0.8452\n",
            "Epoch 51/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7699 - loss: 0.8239\n",
            "Epoch 52/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7770 - loss: 0.7938\n",
            "Epoch 53/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7761 - loss: 0.8009\n",
            "Epoch 54/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7803 - loss: 0.7922\n",
            "Epoch 55/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7795 - loss: 0.7963\n",
            "Epoch 56/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7802 - loss: 0.7826\n",
            "Epoch 57/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7738 - loss: 0.8077\n",
            "Epoch 58/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7773 - loss: 0.7940\n",
            "Epoch 59/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7757 - loss: 0.7923\n",
            "Epoch 60/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7789 - loss: 0.7802\n",
            "Epoch 61/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7862 - loss: 0.7546\n",
            "Epoch 62/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7833 - loss: 0.7575\n",
            "Epoch 63/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7886 - loss: 0.7617\n",
            "Epoch 64/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7841 - loss: 0.7727\n",
            "Epoch 65/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7907 - loss: 0.7477\n",
            "Epoch 66/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7905 - loss: 0.7589\n",
            "Epoch 67/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7789 - loss: 0.7645\n",
            "Epoch 68/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7897 - loss: 0.7414\n",
            "Epoch 69/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7803 - loss: 0.7667\n",
            "Epoch 70/100\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7823 - loss: 0.7517\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a11210832e0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Prediction"
      ],
      "metadata": {
        "id": "gL8MsJXqDuv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_prompt(prompt, num_words, model, tokenizer, max_sequence_len):\n",
        "    generated_text = prompt\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        # Preprocess the prompt\n",
        "        prompt_proc = clean_and_normalize_text(generated_text)\n",
        "        prompt_proc = tokenizer.texts_to_sequences([prompt_proc])[0]\n",
        "        prompt_proc = pad_sequences([prompt_proc], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predict = model.predict(prompt_proc, verbose=0)\n",
        "        predicted_index = np.argmax(predict, axis=1)[0]\n",
        "\n",
        "        # Convert predicted index to word\n",
        "        next_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                next_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the generated text\n",
        "        generated_text += \" \" + next_word\n",
        "\n",
        "    return generated_text.title()"
      ],
      "metadata": {
        "id": "KyfMQCN5Fjge"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `generate_text_from_prompt` function is designed to generate text based on an initial prompt provided by the user. Here's how it works:\n",
        "\n",
        "* **Starting Point:** The function begins with a **user-provided prompt**, which serves as the seed for generating new text. This prompt is passed to the function along with the number of words to generate, the trained model, the tokenizer used during training, and the maximum sequence length the model expects.\n",
        "\n",
        "* **Text Preparation:** Before the model can make predictions, the prompt needs to be preprocessed. This involves cleaning the text by removing punctuation and converting it to lowercase. The preprocessed text is then tokenized using the same tokenizer that was used to train the model. This process transforms the text into sequences of integers that represent words.\n",
        "\n",
        "* **Generating Words:** With the processed prompt, the function then prepares the input for the model. It does this by padding the sequence to ensure it matches the length expected by the model. Padding is crucial because the model requires a fixed input size, and padding helps to standardize the length of sequences.\n",
        "\n",
        "* **Making Predictions:** The model takes this padded sequence and predicts the next word in the sequence. The output is a probability distribution over the vocabulary, indicating the likelihood of each word being the next one. The function identifies the word with the highest probability as the predicted next word.\n",
        "\n",
        "* **Mapping Index to Word:** After predicting the next word, the function needs to convert the predicted index back into the actual word. It does this by looking up the word index dictionary provided by the tokenizer, which maps integer indices to their corresponding words.\n",
        "\n",
        "* **Appending the Word:** The newly predicted word is then appended to the current text, extending the prompt with the generated word. This updated text now becomes the new prompt for generating subsequent words.\n",
        "\n",
        "* **Repeating the Process:** The function repeats this process of predicting the next word and appending it to the text for the number of words specified by the user. This iterative process gradually builds up the text based on the initial prompt.\n",
        "\n",
        "* **Formatting the Output:** Once the desired number of words has been generated, the function formats the final text by capitalizing the first letter of each word, providing a cleaner and more readable output.\n",
        "\n",
        "In essence, this function is like a creative writing assistant that takes a starting sentence and continues to build upon it word by word. By leveraging the trained model's ability to predict the next word in a sequence, it generates coherent and contextually relevant text based on the initial prompt. This is particularly useful in applications like text completion, story generation, or any scenario where automated text creation is required."
      ],
      "metadata": {
        "id": "C-msO7WlDu6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text_from_prompt(\"Cybersecurity\", 5, model, tokenizer, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nVYD4tdTHhY",
        "outputId": "c91422b6-968e-4c2b-fd7f-9d0eef22c445"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cybersecurity Professionals Defending Our Privacy Defense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text_from_prompt(\"Artificial Intelligence\", 3, model, tokenizer, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z2uTGg7UNOw",
        "outputId": "4eb71df7-2984-469a-f157-2fa5fb689dbd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Intelligence And The Revolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text_from_prompt(\"Future\", 7, model, tokenizer, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbYnfgM3UliW",
        "outputId": "c3074499-1c21-4de9-8fc9-0e4e4e085d03"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Future Education As A Tool For Global Access\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text_from_prompt(\"Blockchain\", 8, model, tokenizer, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM5P0a3tU8n-",
        "outputId": "3a2beb46-4518-4432-e620-a8749fc0eb46"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blockchain Technology And Its Impact On The Financial Industry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text_from_prompt(\"Automobiles\", 6, model, tokenizer, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LN7KWtjVCX-",
        "outputId": "922dc3ad-14b3-45dd-a459-8cc098e4afef"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automobiles And Green Infrastructure In Urban Stormwater\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Trained Model and Weights"
      ],
      "metadata": {
        "id": "-RhqY6cES03Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training a deep learning model, it's essential to save both the model architecture and its trained weights for future use or deployment. We can achieve this in TensorFlow using the save method provided by the Keras API."
      ],
      "metadata": {
        "id": "e16uUvSqS6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file paths for saving the model and weights\n",
        "model_path = 'trained_model.model.h5'\n",
        "weights_path = 'trained_model.weights.h5'\n",
        "\n",
        "# Save the trained model architecture\n",
        "model.save(model_path)\n",
        "\n",
        "# Save the trained model weights\n",
        "model.save_weights(weights_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVt2uVQ1S1B3",
        "outputId": "37e94660-68f0-4aa7-dfbd-b18b9a888581"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code saves the entire model architecture to a single HDF5 file (model.h5) and the trained weights to another HDF5 file (weights.h5). These files can then be loaded later to make predictions on new data or continue training the model."
      ],
      "metadata": {
        "id": "pFB4pF1yS__3"
      }
    }
  ]
}