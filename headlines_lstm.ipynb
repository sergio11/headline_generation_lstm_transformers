{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Headlines in English Using LSTM"
      ],
      "metadata": {
        "id": "SN4SwHk9KYEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In todayâ€™s fast-paced digital world, the ability to create compelling and relevant headlines is crucial for capturing audience attention and driving engagement. Headlines serve as the first impression of content, influencing readers' decisions to explore articles further. Given the growing volume of content and the demand for timely information, automating the headline creation process presents a significant opportunity.\n",
        "\n",
        "This project addresses this need by leveraging advanced machine learning techniques to automate the generation of headlines. The core of this approach is based on Long **Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN)** renowned for its ability to handle sequences and long-term dependencies in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "b1YXGqeYKYUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why LSTM for Headline Generation?"
      ],
      "metadata": {
        "id": "Z7CW3CElKYWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional algorithms for text generation often struggle with maintaining coherence over longer sequences, leading to headlines that may lack relevance or readability. LSTMs, with their specialized architecture, are designed to remember and use contextual information from earlier parts of the sequence. This makes them particularly effective for generating text that is not only grammatically correct but also contextually appropriate."
      ],
      "metadata": {
        "id": "GJEQlJbpLX7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals"
      ],
      "metadata": {
        "id": "DGaPlp7PL4lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to develop an **LSTM-based model** that can generate high-quality, engaging headlines in English. By training the model on a diverse dataset of existing headlines, we aim to produce headlines that are not only accurate but also creative and relevant. This model has the potential to assist content creators, journalists, and marketers by providing them with a tool to quickly generate impactful headlines, thereby enhancing productivity and content engagement."
      ],
      "metadata": {
        "id": "aIBJNtDGL6jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Reading the dataset"
      ],
      "metadata": {
        "id": "4HhcL-SmJ5W8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jtjo41iWCthm"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/dataset.txt\", encoding=\"latin-1\") as f:\n",
        "    dataset = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi6EezaKJxD5",
        "outputId": "f4a21a44-b46a-42f1-a44c-b5b6c466d1fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['New energy law promises to revolutionize the electric sector',\n",
              " 'Climate change continues to be a global threat',\n",
              " 'Investors seek opportunities in renewable energy',\n",
              " 'Demand for electric vehicles increases',\n",
              " 'COVID-19 vaccines: When will we all be protected?',\n",
              " 'The debate over vaccines continues to divide opinions',\n",
              " 'Health experts analyze the effectiveness of vaccines',\n",
              " 'Mass vaccination against coronavirus underway',\n",
              " 'Cryptocurrency market soars to new heights',\n",
              " 'Is Bitcoin the currency of the future?']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation"
      ],
      "metadata": {
        "id": "Tpsjv0-nMdtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Data Cleaning"
      ],
      "metadata": {
        "id": "2xk0OOFQMd2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import unicodedata\n",
        "\n",
        "def clean_and_normalize_text(txt):\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    txt = \"\".join(c for c in txt if c not in string.punctuation).lower()\n",
        "    # Normalize unicode characters and encode to ASCII\n",
        "    txt = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return txt"
      ],
      "metadata": {
        "id": "TBT27anGMeHW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [clean_and_normalize_text(headline) for headline in dataset]"
      ],
      "metadata": {
        "id": "aLxrlsArN_JV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag3oEpUGOLFD",
        "outputId": "a41a4e37-5576-4b43-9ad2-fe04939cae14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['new energy law promises to revolutionize the electric sector',\n",
              " 'climate change continues to be a global threat',\n",
              " 'investors seek opportunities in renewable energy',\n",
              " 'demand for electric vehicles increases',\n",
              " 'covid19 vaccines when will we all be protected',\n",
              " 'the debate over vaccines continues to divide opinions',\n",
              " 'health experts analyze the effectiveness of vaccines',\n",
              " 'mass vaccination against coronavirus underway',\n",
              " 'cryptocurrency market soars to new heights',\n",
              " 'is bitcoin the currency of the future']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `clean_and_normalize_text` is designed to prepare text data for further processing by cleaning and standardizing it. This is a crucial step in text analysis and natural language processing. The function accomplishes the following:\n",
        "\n",
        "**Remove Unwanted Characters:**\n",
        "\n",
        "* **Objective:** Eliminate punctuation marks from the text.\n",
        "* **Why:** Punctuation can interfere with text analysis tasks such as text classification or tokenization. Removing it **helps in focusing on the core content of the text.**\n",
        "\n",
        "**Standardize Text:**\n",
        "\n",
        "* **Objective:** Normalize the text by converting it to lowercase and removing any special or accented characters.\n",
        "* **Why:** Converting the text to lowercase ensures uniformity, as \"Hello\" and \"hello\" would be treated as the same word. Normalizing accents and special characters helps in handling text from different sources and languages consistently, making it easier to analyze and compare.\n",
        "\n",
        "`clean_and_normalize_text` transforms raw text into a cleaner, more uniform format. This preprocessing step is essential for effective text analysis, improving the accuracy and reliability of subsequent processing tasks such as machine learning model training or text-based querying."
      ],
      "metadata": {
        "id": "ByDmBRcIM9QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Data Tokenization"
      ],
      "metadata": {
        "id": "rRPyBwKAOU8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "aKwmnUFSOZz5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_token_sequences(tokenizer, dataset):\n",
        "    # Build the tokenizer\n",
        "    tokenizer.fit_on_texts(dataset)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Tokenize the text in the dataset\n",
        "    dataset_tokens = []\n",
        "    for text in dataset:\n",
        "        text_tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        # Generate n-grams from the tokenized text\n",
        "        for i in range(1, len(text_tokens)):\n",
        "            n_gram = text_tokens[:i+1]\n",
        "            dataset_tokens.append(n_gram)\n",
        "\n",
        "    return dataset_tokens, total_words\n"
      ],
      "metadata": {
        "id": "pAQjPyKsO_1d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_tokens, total_words = generate_token_sequences(tokenizer, dataset)"
      ],
      "metadata": {
        "id": "LZqeNrTIPkAG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `generate_token_sequences` is used to preprocess a text dataset by tokenizing the text and generating n-grams. It first builds a tokenizer based on the dataset, then converts each text into sequences of integers, and finally creates and collects various n-grams (sub-sequences) from the tokenized text. This preprocessing step is essential for transforming raw text data into a structured format **suitable for training machine learning models or performing further text analysis.**"
      ],
      "metadata": {
        "id": "BY7nrcsPPDrq"
      }
    }
  ]
}