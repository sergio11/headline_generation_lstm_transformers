{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Headlines in English Using Transformers"
      ],
      "metadata": {
        "id": "ZBea5a-aHdeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s fast-paced digital landscape, the ability to create engaging and relevant headlines is essential for capturing audience attention and driving interaction. Headlines are often the first encounter readers have with content, and their effectiveness can significantly influence whether an article is read further. With the increasing volume of content and the need for timely updates, automating the headline generation process has become an important opportunity.\n",
        "\n",
        "This project aims to address this challenge by utilizing **cutting-edge Transformer architecture**, a powerful machine learning model designed for processing sequences. Unlike traditional models, **Transformers leverage self-attention mechanisms to efficiently handle long-range dependencies and capture intricate relationships within the data**. This approach enables the model to generate coherent and contextually relevant headlines by understanding and integrating the nuances of the input text, ultimately enhancing the automation and quality of headline creation."
      ],
      "metadata": {
        "id": "7UDBgSl7Hdhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transformers for Headline Generation?"
      ],
      "metadata": {
        "id": "8z_5wWqLMZes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional algorithms for text generation often face challenges in capturing and maintaining long-range dependencies within the text, which can result in headlines that lack coherence or relevance. Transformers, with their self-attention mechanism, are designed to address this issue effectively. Unlike earlier models, **Transformers can analyze and integrate information from different parts of the sequence simultaneously**, allowing them to generate headlines that are not only grammatically sound but also contextually nuanced and relevant. This capability makes Transformers particularly suited for generating compelling and coherent headlines, even over long sequences of text."
      ],
      "metadata": {
        "id": "So914HhoMbz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals"
      ],
      "metadata": {
        "id": "xqzyyWNYMsbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to develop a **Transformer-based** model capable of generating high-quality, engaging headlines in English. By training the model on a comprehensive dataset of existing headlines, we seek to produce headlines that are both accurate and creatively crafted. The Transformer’s advanced architecture will enable the generation of headlines that are contextually rich and impactful. This model is intended to support content creators, journalists, and marketers by offering a powerful tool to quickly generate captivating headlines, ultimately boosting productivity and enhancing content engagement."
      ],
      "metadata": {
        "id": "OQI1ckPHMvND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "pDKowPfdHeTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `causal_attention_mask` function creates a mask that ensures each word in a sequence can only be influenced by previous words and itself, not by future words. This is crucial for text generation models, as it prevents the model from using future information to make predictions, ensuring that the generated text is coherent and correctly ordered."
      ],
      "metadata": {
        "id": "8btdKYnyIK67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Creates a causal attention mask to ensure that each token in a sequence only attends to previous and current tokens,\n",
        "    but not to future tokens. This is crucial for autoregressive models where each token should not be influenced\n",
        "    by tokens that come after it in the sequence.\n",
        "\n",
        "    The mask is designed to be applied to the attention weights in self-attention mechanisms, such as those used\n",
        "    in Transformer models. It prevents information from flowing from future tokens to the current token, ensuring\n",
        "    that predictions for each token depend only on tokens that precede it.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of sequences in the batch.\n",
        "    - n_dest (int): The length of the destination sequence (number of tokens in the sequence being processed).\n",
        "    - n_src (int): The length of the source sequence (typically equal to n_dest in self-attention).\n",
        "    - dtype (tf.DType): The data type for the mask tensor (e.g., tf.float32, tf.int32).\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] where the upper triangle of the dot product matrix\n",
        "      is masked out with zeros, and the lower triangle (including the diagonal) is filled with ones. This tensor\n",
        "      can be used to mask the attention weights in a self-attention mechanism, ensuring that each token attends only\n",
        "      to earlier tokens and itself, but not to future tokens.\n",
        "\n",
        "    Example:\n",
        "    >>> causal_mask = causal_attention_mask(2, 4, 4, tf.float32)\n",
        "    >>> print(causal_mask)\n",
        "    <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=\n",
        "    array([[[1., 0., 0., 0.],\n",
        "            [1., 1., 0., 0.],\n",
        "            [1., 1., 1., 0.],\n",
        "            [1., 1., 1., 1.]],\n",
        "\n",
        "           [[1., 0., 0., 0.],\n",
        "            [1., 1., 0., 0.],\n",
        "            [1., 1., 1., 0.],\n",
        "            [1., 1., 1., 1.]]], dtype=float32)>\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "TKdElQ7SHjIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TransformerBlock` class is a fundamental building block in modern deep learning models, especially those designed for handling sequences, like text. Imagine you're building a sophisticated machine that can understand and generate language—like a powerful translator or a smart chatbot. This machine needs to process sequences of words and understand the relationships between them. That’s where the TransformerBlock comes in."
      ],
      "metadata": {
        "id": "0q9cxET4Isqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    A single block of the Transformer model architecture. This block combines multi-head self-attention\n",
        "    and feed-forward neural networks to process input sequences.\n",
        "\n",
        "    The TransformerBlock is designed to capture complex dependencies in sequential data by using self-attention\n",
        "    mechanisms. It also includes feed-forward layers to further process the attention outputs, along with normalization\n",
        "    and dropout layers to stabilize training and prevent overfitting.\n",
        "\n",
        "    Attributes:\n",
        "    - embed_dim (int): The dimension of the embedding space.\n",
        "    - num_heads (int): The number of attention heads in the multi-head attention mechanism.\n",
        "    - ff_dim (int): The dimension of the feed-forward network hidden layer.\n",
        "    - rate (float): The dropout rate applied to the attention and feed-forward layers (default is 0.1).\n",
        "\n",
        "    Methods:\n",
        "    - call(inputs): Executes the forward pass of the Transformer block. It applies the multi-head attention, adds\n",
        "      residual connections, normalizes the outputs, and processes them through a feed-forward network.\n",
        "\n",
        "    Parameters:\n",
        "    - inputs (tf.Tensor): Input tensor with shape (batch_size, seq_len, embed_dim). Represents the sequence of embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: Output tensor with shape (batch_size, seq_len, embed_dim). The processed sequence after attention,\n",
        "      feed-forward operations, and normalization.\n",
        "\n",
        "    Example:\n",
        "    >>> transformer_block = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128)\n",
        "    >>> inputs = tf.random.uniform((32, 10, 64))  # Example input tensor with batch_size=32, seq_len=10, embed_dim=64\n",
        "    >>> output = transformer_block(inputs)\n",
        "    >>> print(output.shape)\n",
        "    (32, 10, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "metadata": {
        "id": "ExS76pJLIbz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At its core, the `TransformerBlock` is designed to help the machine focus on different parts of a sequence when making decisions. For example, if you’re translating a sentence, the model needs to understand which words in the sentence are related to each other, even if they are far apart. The TransformerBlock achieves this using two **key mechanisms: attention and feed-forward processing.**"
      ],
      "metadata": {
        "id": "S8wc7FkWIujj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Self-Attention Mechanism:**\n",
        "Think of this as the block's way of looking at all the words in a sentence and figuring out which words should be paid more attention to. It does this by calculating how each word relates to every other word in the sequence. This process is known as self-attention. For instance, in the sentence \"The cat sat on the mat,\" the **TransformerBlock** helps the model understand that \"cat\" and \"mat\" are related, even though they are not next to each other.\n",
        "\n",
        "* **Feed-Forward Network:**\n",
        "After understanding these relationships, the next step is to further process this **information to make it more useful**. This is where the feed-forward network comes in. **It takes the attention outputs and applies additional transformations to refine the information**. **This step helps in capturing complex patterns and making the final output more precise.**\n",
        "\n",
        "* **Normalization and Dropout:**\n",
        "To **ensure the model learns effectively and doesn’t overfit to the training data**, the TransformerBlock includes normalization and dropout layers. Normalization helps in stabilizing the training process by adjusting the outputs to a standard scale. Dropout, on the other hand, randomly \"drops\" some of the data during training to **prevent the model from becoming too dependent on any specific part of the input.**"
      ],
      "metadata": {
        "id": "wR6DmVORIzBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, the `TransformerBlock` acts as a smart processor within a larger machine learning model. It helps the model focus on important parts of the input sequence, processes this information in a sophisticated way, and ensures that the learning process is stable and robust. This makes it an essential component in creating models that can understand and generate human-like text, perform translations, or even respond intelligently in conversations."
      ],
      "metadata": {
        "id": "kbmNXHK5I_qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TokenAndPositionEmbedding` class is designed to convert sequences of tokens into meaningful numerical representations for a machine learning model. **This class addresses two key aspects:**\n",
        "\n",
        "* **Token Representation:** It translates each token in the input sequence into a **dense vector**, known as a token embedding. **This vector captures the semantic information of the token**, allowing the model to interpret its meaning.\n",
        "\n",
        "* **Positional Information:** To capture the order of tokens within the sequence, it generates **positional embeddings**. These embeddings encode the position of each token, ensuring that the model understands the sequential context and the relative positioning of tokens.\n",
        "\n",
        "By combining **token embeddings with positional embeddings, the class provides a comprehensive representation of each token that includes both its meaning and its position in the sequence**. This combined representation is crucial for models to process and understand sequences accurately, enhancing their ability to perform tasks that depend on the order and context of the tokens."
      ],
      "metadata": {
        "id": "ons34PvlKbLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that combines token embeddings and positional embeddings for sequences.\n",
        "    This layer is designed to convert input tokens into dense vectors and add positional information\n",
        "    to each token embedding to capture the order of tokens in a sequence.\n",
        "\n",
        "    The TokenAndPositionEmbedding layer is crucial for models that process sequential data, such as\n",
        "    natural language processing models, where understanding the position of each token in the sequence\n",
        "    is essential for interpreting the context and meaning.\n",
        "\n",
        "    Attributes:\n",
        "    - maxlen (int): The maximum length of the input sequences. This determines the size of the positional\n",
        "      embeddings.\n",
        "    - vocab_size (int): The size of the vocabulary, which determines the number of possible tokens.\n",
        "    - embed_dim (int): The dimensionality of the embedding space. Each token and position is mapped to a vector of\n",
        "      this size.\n",
        "\n",
        "    Methods:\n",
        "    - call(x): Applies the token and positional embeddings to the input sequences. It generates embeddings for each\n",
        "      token and adds positional embeddings to these token embeddings to encode the order of tokens in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - x (tf.Tensor): Input tensor of shape (batch_size, sequence_length), where each value represents a token index\n",
        "      in the input sequences.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: Output tensor of shape (batch_size, sequence_length, embed_dim), where each token index in the input\n",
        "      sequences has been converted into an embedding vector, with positional information added to it.\n",
        "\n",
        "    Example:\n",
        "    >>> embedding_layer = TokenAndPositionEmbedding(maxlen=100, vocab_size=5000, embed_dim=64)\n",
        "    >>> input_seq = tf.constant([[1, 5, 9], [2, 6, 3]])\n",
        "    >>> output = embedding_layer(input_seq)\n",
        "    >>> print(output.shape)\n",
        "    (2, 3, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "41WiY0hfKbak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}