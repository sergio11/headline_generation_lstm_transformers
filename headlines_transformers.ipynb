{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Headlines in English Using Transformers"
      ],
      "metadata": {
        "id": "ZBea5a-aHdeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s fast-paced digital landscape, the ability to create engaging and relevant headlines is essential for capturing audience attention and driving interaction. Headlines are often the first encounter readers have with content, and their effectiveness can significantly influence whether an article is read further. With the increasing volume of content and the need for timely updates, automating the headline generation process has become an important opportunity.\n",
        "\n",
        "This project aims to address this challenge by utilizing **cutting-edge Transformer architecture**, a powerful machine learning model designed for processing sequences. Unlike traditional models, **Transformers leverage self-attention mechanisms to efficiently handle long-range dependencies and capture intricate relationships within the data**. This approach enables the model to generate coherent and contextually relevant headlines by understanding and integrating the nuances of the input text, ultimately enhancing the automation and quality of headline creation."
      ],
      "metadata": {
        "id": "7UDBgSl7Hdhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transformers for Headline Generation?"
      ],
      "metadata": {
        "id": "8z_5wWqLMZes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional algorithms for text generation often face challenges in capturing and maintaining long-range dependencies within the text, which can result in headlines that lack coherence or relevance. Transformers, with their self-attention mechanism, are designed to address this issue effectively. Unlike earlier models, **Transformers can analyze and integrate information from different parts of the sequence simultaneously**, allowing them to generate headlines that are not only grammatically sound but also contextually nuanced and relevant. This capability makes Transformers particularly suited for generating compelling and coherent headlines, even over long sequences of text."
      ],
      "metadata": {
        "id": "So914HhoMbz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals"
      ],
      "metadata": {
        "id": "xqzyyWNYMsbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to develop a **Transformer-based** model **capable of generating high-quality, engaging headlines in English**. By training the model on a comprehensive dataset of existing headlines, we seek to produce headlines that are both accurate and creatively crafted. The Transformer’s advanced architecture will enable the generation of headlines that are contextually rich and impactful. This model is intended to support content creators, journalists, and marketers by offering a powerful tool to quickly generate captivating headlines, ultimately boosting productivity and enhancing content engagement."
      ],
      "metadata": {
        "id": "OQI1ckPHMvND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "pDKowPfdHeTr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `causal_attention_mask` function creates a mask that ensures each word in a sequence can only be influenced by previous words and itself, not by future words. This is crucial for text generation models, as it prevents the model from using future information to make predictions, ensuring that the generated text is coherent and correctly ordered."
      ],
      "metadata": {
        "id": "8btdKYnyIK67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Creates a causal attention mask to ensure that each token in a sequence only attends to previous and current tokens,\n",
        "    but not to future tokens. This is crucial for autoregressive models where each token should not be influenced\n",
        "    by tokens that come after it in the sequence.\n",
        "\n",
        "    The mask is designed to be applied to the attention weights in self-attention mechanisms, such as those used\n",
        "    in Transformer models. It prevents information from flowing from future tokens to the current token, ensuring\n",
        "    that predictions for each token depend only on tokens that precede it.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of sequences in the batch.\n",
        "    - n_dest (int): The length of the destination sequence (number of tokens in the sequence being processed).\n",
        "    - n_src (int): The length of the source sequence (typically equal to n_dest in self-attention).\n",
        "    - dtype (tf.DType): The data type for the mask tensor (e.g., tf.float32, tf.int32).\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] where the upper triangle of the dot product matrix\n",
        "      is masked out with zeros, and the lower triangle (including the diagonal) is filled with ones. This tensor\n",
        "      can be used to mask the attention weights in a self-attention mechanism, ensuring that each token attends only\n",
        "      to earlier tokens and itself, but not to future tokens.\n",
        "\n",
        "    Example:\n",
        "    >>> causal_mask = causal_attention_mask(2, 4, 4, tf.float32)\n",
        "    >>> print(causal_mask)\n",
        "    <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=\n",
        "    array([[[1., 0., 0., 0.],\n",
        "            [1., 1., 0., 0.],\n",
        "            [1., 1., 1., 0.],\n",
        "            [1., 1., 1., 1.]],\n",
        "\n",
        "           [[1., 0., 0., 0.],\n",
        "            [1., 1., 0., 0.],\n",
        "            [1., 1., 1., 0.],\n",
        "            [1., 1., 1., 1.]]], dtype=float32)>\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "TKdElQ7SHjIb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TransformerBlock` class is a fundamental building block in modern deep learning models, especially those designed for handling sequences, like text. Imagine you're building a sophisticated machine that can understand and generate language—like a powerful translator or a smart chatbot. This machine **needs to process sequences of words and understand the relationships between them**. That’s where the TransformerBlock comes in."
      ],
      "metadata": {
        "id": "0q9cxET4Isqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    A single block of the Transformer model architecture. This block combines multi-head self-attention\n",
        "    and feed-forward neural networks to process input sequences.\n",
        "\n",
        "    The TransformerBlock is designed to capture complex dependencies in sequential data by using self-attention\n",
        "    mechanisms. It also includes feed-forward layers to further process the attention outputs, along with normalization\n",
        "    and dropout layers to stabilize training and prevent overfitting.\n",
        "\n",
        "    Attributes:\n",
        "    - embed_dim (int): The dimension of the embedding space.\n",
        "    - num_heads (int): The number of attention heads in the multi-head attention mechanism.\n",
        "    - ff_dim (int): The dimension of the feed-forward network hidden layer.\n",
        "    - rate (float): The dropout rate applied to the attention and feed-forward layers (default is 0.1).\n",
        "\n",
        "    Methods:\n",
        "    - call(inputs): Executes the forward pass of the Transformer block. It applies the multi-head attention, adds\n",
        "      residual connections, normalizes the outputs, and processes them through a feed-forward network.\n",
        "\n",
        "    Parameters:\n",
        "    - inputs (tf.Tensor): Input tensor with shape (batch_size, seq_len, embed_dim). Represents the sequence of embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: Output tensor with shape (batch_size, seq_len, embed_dim). The processed sequence after attention,\n",
        "      feed-forward operations, and normalization.\n",
        "\n",
        "    Example:\n",
        "    >>> transformer_block = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128)\n",
        "    >>> inputs = tf.random.uniform((32, 10, 64))  # Example input tensor with batch_size=32, seq_len=10, embed_dim=64\n",
        "    >>> output = transformer_block(inputs)\n",
        "    >>> print(output.shape)\n",
        "    (32, 10, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()  # Initializes the parent class (layers.Layer).\n",
        "        # MultiHeadAttention layer to capture relationships between different positions in the sequence.\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "\n",
        "        # Feed-forward network with a ReLU activation function followed by a linear layer.\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),  # Dense layer with ReLU activation.\n",
        "                layers.Dense(embed_dim),  # Dense layer to project back to the embedding dimension.\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Layer normalization applied before and after the residual connection.\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)  # First layer normalization.\n",
        "\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)  # Second layer normalization.\n",
        "\n",
        "        # Dropout layers to prevent overfitting by randomly setting a fraction of input units to zero.\n",
        "        self.dropout1 = layers.Dropout(rate)  # Dropout after the attention layer.\n",
        "\n",
        "        self.dropout2 = layers.Dropout(rate)  # Dropout after the feed-forward network.\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the Transformer block.\n",
        "\n",
        "        Arguments:\n",
        "        inputs -- The input tensor to the Transformer block.\n",
        "\n",
        "        Returns:\n",
        "        The output tensor of the Transformer block after applying attention, dropout, and feed-forward network.\n",
        "        \"\"\"\n",
        "        input_shape = tf.shape(inputs)  # Get the shape of the input tensor.\n",
        "        batch_size = input_shape[0]  # Number of sequences in the batch.\n",
        "        seq_len = input_shape[1]  # Length of each sequence.\n",
        "\n",
        "        # Create a causal attention mask to prevent attending to future tokens.\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "\n",
        "        # Apply multi-head attention with the causal mask.\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "\n",
        "        # Apply dropout to the attention output.\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "\n",
        "        # Add the input (residual connection) to the attention output and normalize.\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "\n",
        "        # Apply the feed-forward network to the normalized output.\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        # Apply dropout to the feed-forward network output.\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "\n",
        "        # Add the normalized output of the feed-forward network to the residual connection and normalize.\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "metadata": {
        "id": "ExS76pJLIbz7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At its core, the `TransformerBlock` is designed to help the machine focus on different parts of a sequence when making decisions. For example, if you’re translating a sentence, the model needs to understand which words in the sentence are related to each other, even if they are far apart. The TransformerBlock achieves this using two **key mechanisms: attention and feed-forward processing.**"
      ],
      "metadata": {
        "id": "S8wc7FkWIujj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Self-Attention Mechanism:**\n",
        "Think of this as the block's way of looking at all the words in a sentence and figuring out which words should be paid more attention to. It does this by **calculating how each word relates to every other word in the sequence**. This process is known as self-attention. For instance, in the sentence \"The cat sat on the mat,\" the **TransformerBlock** helps the model understand that \"cat\" and \"mat\" are related, even though they are not next to each other.\n",
        "\n",
        "* **Feed-Forward Network:**\n",
        "After understanding these relationships, the next step is to further process this **information to make it more useful**. This is where the feed-forward network comes in. **It takes the attention outputs and applies additional transformations to refine the information**. **This step helps in capturing complex patterns and making the final output more precise.**\n",
        "\n",
        "* **Normalization and Dropout:**\n",
        "To **ensure the model learns effectively and doesn’t overfit to the training data**, the TransformerBlock includes normalization and dropout layers. Normalization helps in stabilizing the training process by adjusting the outputs to a standard scale. Dropout, on the other hand, randomly \"drops\" some of the data during training to **prevent the model from becoming too dependent on any specific part of the input.**"
      ],
      "metadata": {
        "id": "wR6DmVORIzBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, the `TransformerBlock` acts as a smart processor within a larger machine learning model. It helps the model focus on important parts of the input sequence, processes this information in a sophisticated way, and ensures that the learning process is stable and robust. This makes it an essential component in creating models that can understand and generate human-like text, perform translations, or even respond intelligently in conversations."
      ],
      "metadata": {
        "id": "kbmNXHK5I_qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TokenAndPositionEmbedding` class is designed to convert sequences of tokens into meaningful numerical representations for a machine learning model. **This class addresses two key aspects:**\n",
        "\n",
        "* **Token Representation:** It translates each token in the input sequence into a **dense vector**, known as a token embedding. **This vector captures the semantic information of the token**, allowing the model to interpret its meaning.\n",
        "\n",
        "* **Positional Information:** To capture the order of tokens within the sequence, it generates **positional embeddings**. These embeddings encode the position of each token, ensuring that the model understands the sequential context and the relative positioning of tokens.\n",
        "\n",
        "By combining **token embeddings with positional embeddings, the class provides a comprehensive representation of each token that includes both its meaning and its position in the sequence**. This combined representation is crucial for models to process and understand sequences accurately, enhancing their ability to perform tasks that depend on the order and context of the tokens."
      ],
      "metadata": {
        "id": "ons34PvlKbLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that combines token embeddings and positional embeddings for sequences.\n",
        "    This layer is designed to convert input tokens into dense vectors and add positional information\n",
        "    to each token embedding to capture the order of tokens in a sequence.\n",
        "\n",
        "    The TokenAndPositionEmbedding layer is crucial for models that process sequential data, such as\n",
        "    natural language processing models, where understanding the position of each token in the sequence\n",
        "    is essential for interpreting the context and meaning.\n",
        "\n",
        "    Attributes:\n",
        "    - maxlen (int): The maximum length of the input sequences. This determines the size of the positional\n",
        "      embeddings.\n",
        "    - vocab_size (int): The size of the vocabulary, which determines the number of possible tokens.\n",
        "    - embed_dim (int): The dimensionality of the embedding space. Each token and position is mapped to a vector of\n",
        "      this size.\n",
        "\n",
        "    Methods:\n",
        "    - call(x): Applies the token and positional embeddings to the input sequences. It generates embeddings for each\n",
        "      token and adds positional embeddings to these token embeddings to encode the order of tokens in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - x (tf.Tensor): Input tensor of shape (batch_size, sequence_length), where each value represents a token index\n",
        "      in the input sequences.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: Output tensor of shape (batch_size, sequence_length, embed_dim), where each token index in the input\n",
        "      sequences has been converted into an embedding vector, with positional information added to it.\n",
        "\n",
        "    Example:\n",
        "    >>> embedding_layer = TokenAndPositionEmbedding(maxlen=100, vocab_size=5000, embed_dim=64)\n",
        "    >>> input_seq = tf.constant([[1, 5, 9], [2, 6, 3]])\n",
        "    >>> output = embedding_layer(input_seq)\n",
        "    >>> print(output.shape)\n",
        "    (2, 3, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()  # Initializes the parent class (layers.Layer).\n",
        "\n",
        "        # Embedding layer for tokens, maps token indices to dense vectors of size `embed_dim`.\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "\n",
        "        # Embedding layer for positional encodings, maps position indices to dense vectors of size `embed_dim`.\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Applies token and positional embeddings to the input tensor.\n",
        "\n",
        "        Arguments:\n",
        "        x -- The input tensor containing token indices.\n",
        "\n",
        "        Returns:\n",
        "        The tensor after adding token and positional embeddings.\n",
        "        \"\"\"\n",
        "        maxlen = tf.shape(x)[-1]  # Get the length of the sequences from the input tensor shape.\n",
        "\n",
        "        # Generate position indices from 0 to maxlen - 1.\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "\n",
        "        # Apply the positional embedding layer to position indices.\n",
        "        positions = self.pos_emb(positions)\n",
        "\n",
        "        # Apply the token embedding layer to the input tensor.\n",
        "        x = self.token_emb(x)\n",
        "\n",
        "        # Add the token embeddings and positional embeddings.\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "41WiY0hfKbak"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Reading and Preprocessing"
      ],
      "metadata": {
        "id": "BGkOfClbOi1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "pBeS9qL4PAVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Path to the dataset file\n",
        "file = \"/content/dataset.txt\"\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file):\n",
        "    # Create a dataset from the text file\n",
        "    text_ds = tf.data.TextLineDataset(file)\n",
        "    text_ds = text_ds.shuffle(buffer_size=256)\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "else:\n",
        "    # Raise a FileNotFoundError with a descriptive message\n",
        "    raise FileNotFoundError(f\"The file at {file} does not exist.\")"
      ],
      "metadata": {
        "id": "ZqiK7pOwOi_N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `custom_standardization` function is designed to prepare text data for further processing by cleaning it up in a few key ways. Imagine you’re working with a collection of text that needs to be uniformly formatted to ensure consistency before feeding it into a machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "QmXJ2E4BPeb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")"
      ],
      "metadata": {
        "id": "C_RgQt-HPGyM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how the function does this:\n",
        "\n",
        "* **Lowercasing:** First, the function converts all characters in the text to lowercase. This step is crucial because it ensures that the text is uniform—\"Hello\" and \"hello\" will be treated as the same word. **This simplification helps the model handle text more effectively without being thrown off by differences in capitalization.**\n",
        "\n",
        "* **Removing HTML Line-Break Tags:** Next, the function targets HTML line-break tags (<br />) which often appear in web-scraped text or HTML content. These tags are meant to indicate a new line, but they don’t carry meaningful content for text analysis. The function replaces these tags with spaces to ensure that line breaks don’t disrupt the flow of the text.\n",
        "\n",
        "* **Handling Punctuation:** Finally, the function deals with punctuation marks. Punctuation can sometimes create issues in text processing, especially if it’s not uniformly handled. The function identifies any punctuation marks and ensures they are surrounded by spaces. This helps in keeping punctuation separate from the words, making it easier to analyze and process the text."
      ],
      "metadata": {
        "id": "Zd2usz5RPjv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, `custom_standardization` prepares raw text by making it consistently formatted and free of unnecessary HTML tags and improperly handled punctuation. This step is fundamental in ensuring that the text is clean and uniform, setting the stage for more effective and accurate analysis or modeling."
      ],
      "metadata": {
        "id": "41lRt4-gP7JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "Z4odCbBpPMZ0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In preparing text data for a machine learning model, this code snippet is setting up a crucial tool for **transforming raw text into a format that the model can use.**\n",
        "\n",
        "First, it defines the **scope of the vocabulary**, choosing to focus on the top 20,000 most frequently occurring words. This helps keep the model’s vocabulary manageable and relevant, filtering out less common terms that might clutter the data. It also establishes that each sequence of text will be no longer than 80 tokens, ensuring that all inputs are of a consistent size.\n",
        "\n",
        "To turn the text into a numerical format, the code creates a TextVectorization layer. This layer will process the text by first cleaning it up using the custom_standardization function, which handles things like removing unnecessary HTML tags and punctuation. Then, it converts the text into integer indices, where each word is mapped to a unique number based on its frequency.\n",
        "\n",
        "The output sequences are slightly longer than the maximum length specified—set to 81 tokens—to accommodate additional training needs, like predicting the next token in a sequence."
      ],
      "metadata": {
        "id": "NFYs_NHjQKmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation of the dictionary/vocabulary\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
      ],
      "metadata": {
        "id": "VD8ySxJ1PPt9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After setting up the text vectorization layer, the next step is to prepare the vocabulary that the model will use.\n",
        "\n",
        "This snippet begins by adapting the **vectorize_layer** to the text data. This process involves analyzing the text dataset to build a vocabulary based on the most frequent words. Essentially, the layer learns which words are present and how often they occur.\n",
        "\n",
        "Once this adaptation is complete, the code retrieves the vocabulary using **vectorize_layer.get_vocabulary()**. This vocabulary is a list where each word is associated with a unique index. This step is crucial because it allows you to convert these indices back into words when interpreting the model's output or debugging the results."
      ],
      "metadata": {
        "id": "yDB8fsO2Qg0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "xV_gt7DDPRU0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `prepare_lm_inputs_labels` function is designed to prepare text data for training a language model by creating input-output pairs that reflect how the model will learn to predict the next word in a sequence.\n",
        "\n",
        "**Here’s how it works:**\n",
        "\n",
        "* **Expanding Dimensions:** The function starts by adding an extra dimension to the text data. This adjustment is necessary for compatibility with the vectorization layer, which expects input in a specific shape.\n",
        "\n",
        "* **Tokenization:** Next, the text is converted into numerical indices using the vectorize_layer. This process transforms each word in the text into its corresponding token ID, creating a sequence of integers.\n",
        "\n",
        "* **Creating Input-Output Pairs:** The function then prepares two key components:\n",
        "\n",
        "* **Inputs (x):** It takes all tokens except the last one in each sequence. These tokens serve as the input for the model.\n",
        "* **Labels (y):** It shifts the sequence by one position to the right, so each token in the sequence corresponds to the next word in the original text. These tokens act as the target output the model should learn to predict.\n",
        "\n",
        "In essence, this function arranges the text data into sequences where the model learns to predict the next word based on the words it has seen so far. This setup is fundamental for training a language model to generate coherent and contextually appropriate text."
      ],
      "metadata": {
        "id": "NOvrzYwyQ73T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "k_rMkgj9PTI0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Transformer Model"
      ],
      "metadata": {
        "id": "__uWJ2H6u2wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model"
      ],
      "metadata": {
        "id": "d9njuiIau25q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up a **Transformer model for sequence processing**. It begins by defining key parameters: **embedding size (embed_dim)**, **number of attention heads (num_heads)**, and **hidden layer size (feed_forward_dim)**.\n",
        "\n",
        "The `create_model` function builds the model with an input layer for token sequences, an embedding layer to convert tokens into dense vectors with positional encodings, and a **TransformerBlock** to capture relationships within the sequence. **The output is then passed through a dense layer to generate predictions.**\n",
        "\n",
        "The model is compiled with the **Adam optimizer** and a loss function for classification, allowing it to learn and predict based on the input data."
      ],
      "metadata": {
        "id": "k7KRB0N_vWV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring Training with a Custom Text Generation Callback"
      ],
      "metadata": {
        "id": "1E_Ojxg6wjXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"Blockchain\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ],
      "metadata": {
        "id": "feeyIJJ4wj5s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TextGenerator` class is a custom callback designed to generate text from a trained model during the training process. It operates in a few key steps:\n",
        "\n",
        "1. **Initialization:** Upon initialization, the class takes several parameters:\n",
        "\n",
        "- **max_tokens:** Number of tokens to generate following the prompt.\n",
        "- **start_tokens:** Initial tokens from which text generation starts.\n",
        "- **index_to_word:** A mapping from token indices to actual words.\n",
        "- **top_k:** Limits sampling to the top k token predictions.\n",
        "- **print_every:** Frequency (in epochs) at which to print the generated text.\n",
        "- **Text Sampling:** The sample_from method selects the next token based on the model's predicted probabilities. It focuses on the top k most likely tokens and samples from them to ensure diversity in the generated text.\n",
        "\n",
        "2. **Token Conversion:** The detokenize method converts token indices back into readable words using the `index_to_word` mapping.\n",
        "\n",
        "3. **Text Generation:** The on_epoch_end method generates text at the end of specified epochs. It continuously predicts and appends new tokens based on the starting prompt and previously generated tokens until the desired number of tokens is reached. The generated text is then printed out.\n",
        "\n",
        "In essence, this callback provides a way to observe and evaluate the progress of text generation during model training by periodically printing out newly generated text samples."
      ],
      "metadata": {
        "id": "dq5eM8CHwk9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model with Real-Time Text Generation Monitoring"
      ],
      "metadata": {
        "id": "Gw94uHumxivo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.fit(text_ds, verbose=2, epochs=100, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD3kG2aWxrEb",
        "outputId": "8bc80a07-545f-45e0-b7ff-ef2252b1ede6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "generated text:\n",
            "[UNK] :   in  and the     the and     for    in           of     of   \n",
            "\n",
            "11/11 - 14s - 1s/step - loss: 6.7069\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "11/11 - 5s - 434ms/step - loss: 2.2774\n",
            "Epoch 3/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the code sets up and begins training the text generation model. First, the `create_model()` function is called to instantiate and configure the model according to the specified architecture and parameters.\n",
        "\n",
        "Once the model is ready, the `model.fit` method is used to train it. The training process involves feeding the text_ds dataset into the model. This dataset has been preprocessed and tokenized to prepare it for training. The training runs for 100 epochs, which means the model will learn from the entire dataset 100 times.\n",
        "\n",
        "To keep track of the model’s progress and assess its performance during training, a custom callback, `text_gen_callback`, is employed. This callback generates text samples at specified intervals, providing real-time feedback on how well the model is learning to generate coherent and relevant text. The verbose=2 setting ensures that detailed information about the training process is logged, helping to monitor and evaluate the model’s performance closely."
      ],
      "metadata": {
        "id": "PLBBSNkUxrOM"
      }
    }
  ]
}