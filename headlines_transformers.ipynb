{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Headlines in English Using Transformers"
      ],
      "metadata": {
        "id": "ZBea5a-aHdeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s fast-paced digital landscape, the ability to create engaging and relevant headlines is essential for capturing audience attention and driving interaction. Headlines are often the first encounter readers have with content, and their effectiveness can significantly influence whether an article is read further. With the increasing volume of content and the need for timely updates, automating the headline generation process has become an important opportunity.\n",
        "\n",
        "This project aims to address this challenge by utilizing **cutting-edge Transformer architecture**, a powerful machine learning model designed for processing sequences. Unlike traditional models, **Transformers leverage self-attention mechanisms to efficiently handle long-range dependencies and capture intricate relationships within the data**. This approach enables the model to generate coherent and contextually relevant headlines by understanding and integrating the nuances of the input text, ultimately enhancing the automation and quality of headline creation."
      ],
      "metadata": {
        "id": "7UDBgSl7Hdhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transformers for Headline Generation?"
      ],
      "metadata": {
        "id": "8z_5wWqLMZes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional algorithms for text generation often face challenges in capturing and maintaining long-range dependencies within the text, which can result in headlines that lack coherence or relevance. Transformers, with their self-attention mechanism, are designed to address this issue effectively. Unlike earlier models, **Transformers can analyze and integrate information from different parts of the sequence simultaneously**, allowing them to generate headlines that are not only grammatically sound but also contextually nuanced and relevant. This capability makes Transformers particularly suited for generating compelling and coherent headlines, even over long sequences of text."
      ],
      "metadata": {
        "id": "So914HhoMbz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goals"
      ],
      "metadata": {
        "id": "xqzyyWNYMsbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to develop a **Transformer-based** model capable of generating high-quality, engaging headlines in English. By training the model on a comprehensive dataset of existing headlines, we seek to produce headlines that are both accurate and creatively crafted. The Transformer’s advanced architecture will enable the generation of headlines that are contextually rich and impactful. This model is intended to support content creators, journalists, and marketers by offering a powerful tool to quickly generate captivating headlines, ultimately boosting productivity and enhancing content engagement."
      ],
      "metadata": {
        "id": "OQI1ckPHMvND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "pDKowPfdHeTr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `causal_attention_mask` function creates a mask that ensures each word in a sequence can only be influenced by previous words and itself, not by future words. This is crucial for text generation models, as it prevents the model from using future information to make predictions, ensuring that the generated text is coherent and correctly ordered."
      ],
      "metadata": {
        "id": "8btdKYnyIK67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Creates a causal attention mask to ensure that each token in a sequence only attends to previous and current tokens,\n",
        "    but not to future tokens. This is crucial for autoregressive models where each token should not be influenced\n",
        "    by tokens that come after it in the sequence.\n",
        "\n",
        "    The mask is designed to be applied to the attention weights in self-attention mechanisms, such as those used\n",
        "    in Transformer models. It prevents information from flowing from future tokens to the current token, ensuring\n",
        "    that predictions for each token depend only on tokens that precede it.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of sequences in the batch.\n",
        "    - n_dest (int): The length of the destination sequence (number of tokens in the sequence being processed).\n",
        "    - n_src (int): The length of the source sequence (typically equal to n_dest in self-attention).\n",
        "    - dtype (tf.DType): The data type for the mask tensor (e.g., tf.float32, tf.int32).\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] where the upper triangle of the dot product matrix\n",
        "      is masked out with zeros, and the lower triangle (including the diagonal) is filled with ones. This tensor\n",
        "      can be used to mask the attention weights in a self-attention mechanism, ensuring that each token attends only\n",
        "      to earlier tokens and itself, but not to future tokens.\n",
        "\n",
        "    Example:\n",
        "    >>> causal_mask = causal_attention_mask(2, 4, 4, tf.float32)\n",
        "    >>> print(causal_mask)\n",
        "    <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=\n",
        "    array([[[1., 0., 0., 0.],\n",
        "            [1., 1., 0., 0.],\n",
        "            [1., 1., 1., 0.],\n",
        "            [1., 1., 1., 1.]],\n",
        "\n",
        "           [[1., 0., 0., 0.],\n",
        "            [1., 1., 0., 0.],\n",
        "            [1., 1., 1., 0.],\n",
        "            [1., 1., 1., 1.]]], dtype=float32)>\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "TKdElQ7SHjIb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TransformerBlock` class is a fundamental building block in modern deep learning models, especially those designed for handling sequences, like text. Imagine you're building a sophisticated machine that can understand and generate language—like a powerful translator or a smart chatbot. This machine needs to process sequences of words and understand the relationships between them. That’s where the TransformerBlock comes in."
      ],
      "metadata": {
        "id": "0q9cxET4Isqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    A single block of the Transformer model architecture. This block combines multi-head self-attention\n",
        "    and feed-forward neural networks to process input sequences.\n",
        "\n",
        "    The TransformerBlock is designed to capture complex dependencies in sequential data by using self-attention\n",
        "    mechanisms. It also includes feed-forward layers to further process the attention outputs, along with normalization\n",
        "    and dropout layers to stabilize training and prevent overfitting.\n",
        "\n",
        "    Attributes:\n",
        "    - embed_dim (int): The dimension of the embedding space.\n",
        "    - num_heads (int): The number of attention heads in the multi-head attention mechanism.\n",
        "    - ff_dim (int): The dimension of the feed-forward network hidden layer.\n",
        "    - rate (float): The dropout rate applied to the attention and feed-forward layers (default is 0.1).\n",
        "\n",
        "    Methods:\n",
        "    - call(inputs): Executes the forward pass of the Transformer block. It applies the multi-head attention, adds\n",
        "      residual connections, normalizes the outputs, and processes them through a feed-forward network.\n",
        "\n",
        "    Parameters:\n",
        "    - inputs (tf.Tensor): Input tensor with shape (batch_size, seq_len, embed_dim). Represents the sequence of embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: Output tensor with shape (batch_size, seq_len, embed_dim). The processed sequence after attention,\n",
        "      feed-forward operations, and normalization.\n",
        "\n",
        "    Example:\n",
        "    >>> transformer_block = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128)\n",
        "    >>> inputs = tf.random.uniform((32, 10, 64))  # Example input tensor with batch_size=32, seq_len=10, embed_dim=64\n",
        "    >>> output = transformer_block(inputs)\n",
        "    >>> print(output.shape)\n",
        "    (32, 10, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "metadata": {
        "id": "ExS76pJLIbz7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At its core, the `TransformerBlock` is designed to help the machine focus on different parts of a sequence when making decisions. For example, if you’re translating a sentence, the model needs to understand which words in the sentence are related to each other, even if they are far apart. The TransformerBlock achieves this using two **key mechanisms: attention and feed-forward processing.**"
      ],
      "metadata": {
        "id": "S8wc7FkWIujj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Self-Attention Mechanism:**\n",
        "Think of this as the block's way of looking at all the words in a sentence and figuring out which words should be paid more attention to. It does this by calculating how each word relates to every other word in the sequence. This process is known as self-attention. For instance, in the sentence \"The cat sat on the mat,\" the **TransformerBlock** helps the model understand that \"cat\" and \"mat\" are related, even though they are not next to each other.\n",
        "\n",
        "* **Feed-Forward Network:**\n",
        "After understanding these relationships, the next step is to further process this **information to make it more useful**. This is where the feed-forward network comes in. **It takes the attention outputs and applies additional transformations to refine the information**. **This step helps in capturing complex patterns and making the final output more precise.**\n",
        "\n",
        "* **Normalization and Dropout:**\n",
        "To **ensure the model learns effectively and doesn’t overfit to the training data**, the TransformerBlock includes normalization and dropout layers. Normalization helps in stabilizing the training process by adjusting the outputs to a standard scale. Dropout, on the other hand, randomly \"drops\" some of the data during training to **prevent the model from becoming too dependent on any specific part of the input.**"
      ],
      "metadata": {
        "id": "wR6DmVORIzBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, the `TransformerBlock` acts as a smart processor within a larger machine learning model. It helps the model focus on important parts of the input sequence, processes this information in a sophisticated way, and ensures that the learning process is stable and robust. This makes it an essential component in creating models that can understand and generate human-like text, perform translations, or even respond intelligently in conversations."
      ],
      "metadata": {
        "id": "kbmNXHK5I_qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TokenAndPositionEmbedding` class is designed to convert sequences of tokens into meaningful numerical representations for a machine learning model. **This class addresses two key aspects:**\n",
        "\n",
        "* **Token Representation:** It translates each token in the input sequence into a **dense vector**, known as a token embedding. **This vector captures the semantic information of the token**, allowing the model to interpret its meaning.\n",
        "\n",
        "* **Positional Information:** To capture the order of tokens within the sequence, it generates **positional embeddings**. These embeddings encode the position of each token, ensuring that the model understands the sequential context and the relative positioning of tokens.\n",
        "\n",
        "By combining **token embeddings with positional embeddings, the class provides a comprehensive representation of each token that includes both its meaning and its position in the sequence**. This combined representation is crucial for models to process and understand sequences accurately, enhancing their ability to perform tasks that depend on the order and context of the tokens."
      ],
      "metadata": {
        "id": "ons34PvlKbLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that combines token embeddings and positional embeddings for sequences.\n",
        "    This layer is designed to convert input tokens into dense vectors and add positional information\n",
        "    to each token embedding to capture the order of tokens in a sequence.\n",
        "\n",
        "    The TokenAndPositionEmbedding layer is crucial for models that process sequential data, such as\n",
        "    natural language processing models, where understanding the position of each token in the sequence\n",
        "    is essential for interpreting the context and meaning.\n",
        "\n",
        "    Attributes:\n",
        "    - maxlen (int): The maximum length of the input sequences. This determines the size of the positional\n",
        "      embeddings.\n",
        "    - vocab_size (int): The size of the vocabulary, which determines the number of possible tokens.\n",
        "    - embed_dim (int): The dimensionality of the embedding space. Each token and position is mapped to a vector of\n",
        "      this size.\n",
        "\n",
        "    Methods:\n",
        "    - call(x): Applies the token and positional embeddings to the input sequences. It generates embeddings for each\n",
        "      token and adds positional embeddings to these token embeddings to encode the order of tokens in the sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - x (tf.Tensor): Input tensor of shape (batch_size, sequence_length), where each value represents a token index\n",
        "      in the input sequences.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: Output tensor of shape (batch_size, sequence_length, embed_dim), where each token index in the input\n",
        "      sequences has been converted into an embedding vector, with positional information added to it.\n",
        "\n",
        "    Example:\n",
        "    >>> embedding_layer = TokenAndPositionEmbedding(maxlen=100, vocab_size=5000, embed_dim=64)\n",
        "    >>> input_seq = tf.constant([[1, 5, 9], [2, 6, 3]])\n",
        "    >>> output = embedding_layer(input_seq)\n",
        "    >>> print(output.shape)\n",
        "    (2, 3, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "41WiY0hfKbak"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Reading and Preprocessing"
      ],
      "metadata": {
        "id": "BGkOfClbOi1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "pBeS9qL4PAVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "\n",
        "file = \"/content/dataset.txt\"\n",
        "\n",
        "# Create a dataset from text files\n",
        "text_ds = tf.data.TextLineDataset(file)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)"
      ],
      "metadata": {
        "id": "ZqiK7pOwOi_N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "silGi0rMPE70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `custom_standardization` function is designed to prepare text data for further processing by cleaning it up in a few key ways. Imagine you’re working with a collection of text that needs to be uniformly formatted to ensure consistency before feeding it into a machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "QmXJ2E4BPeb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")"
      ],
      "metadata": {
        "id": "C_RgQt-HPGyM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how the function does this:\n",
        "\n",
        "* **Lowercasing:** First, the function converts all characters in the text to lowercase. This step is crucial because it ensures that the text is uniform—\"Hello\" and \"hello\" will be treated as the same word. **This simplification helps the model handle text more effectively without being thrown off by differences in capitalization.**\n",
        "\n",
        "* **Removing HTML Line-Break Tags:** Next, the function targets HTML line-break tags (<br />) which often appear in web-scraped text or HTML content. These tags are meant to indicate a new line, but they don’t carry meaningful content for text analysis. The function replaces these tags with spaces to ensure that line breaks don’t disrupt the flow of the text.\n",
        "\n",
        "* **Handling Punctuation:** Finally, the function deals with punctuation marks. Punctuation can sometimes create issues in text processing, especially if it’s not uniformly handled. The function identifies any punctuation marks and ensures they are surrounded by spaces. This helps in keeping punctuation separate from the words, making it easier to analyze and process the text."
      ],
      "metadata": {
        "id": "Zd2usz5RPjv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, `custom_standardization` prepares raw text by making it consistently formatted and free of unnecessary HTML tags and improperly handled punctuation. This step is fundamental in ensuring that the text is clean and uniform, setting the stage for more effective and accurate analysis or modeling."
      ],
      "metadata": {
        "id": "41lRt4-gP7JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "Z4odCbBpPMZ0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In preparing text data for a machine learning model, this code snippet is setting up a crucial tool for **transforming raw text into a format that the model can use.**\n",
        "\n",
        "First, it defines the **scope of the vocabulary**, choosing to focus on the top 20,000 most frequently occurring words. This helps keep the model’s vocabulary manageable and relevant, filtering out less common terms that might clutter the data. It also establishes that each sequence of text will be no longer than 80 tokens, ensuring that all inputs are of a consistent size.\n",
        "\n",
        "To turn the text into a numerical format, the code creates a TextVectorization layer. This layer will process the text by first cleaning it up using the custom_standardization function, which handles things like removing unnecessary HTML tags and punctuation. Then, it converts the text into integer indices, where each word is mapped to a unique number based on its frequency.\n",
        "\n",
        "The output sequences are slightly longer than the maximum length specified—set to 81 tokens—to accommodate additional training needs, like predicting the next token in a sequence."
      ],
      "metadata": {
        "id": "NFYs_NHjQKmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation of the dictionary/vocabulary\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "VD8ySxJ1PPt9",
        "outputId": "906d8321-e4d0-4a18-8499-e30fd6990b0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/dataset.txt; No such file or directory [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ab32a73fbcd7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preparation of the dictionary/vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# To get words back from token indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/dataset.txt; No such file or directory [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After setting up the text vectorization layer, the next step is to prepare the vocabulary that the model will use.\n",
        "\n",
        "This snippet begins by adapting the **vectorize_layer** to the text data. This process involves analyzing the text dataset to build a vocabulary based on the most frequent words. Essentially, the layer learns which words are present and how often they occur.\n",
        "\n",
        "Once this adaptation is complete, the code retrieves the vocabulary using **vectorize_layer.get_vocabulary()**. This vocabulary is a list where each word is associated with a unique index. This step is crucial because it allows you to convert these indices back into words when interpreting the model's output or debugging the results."
      ],
      "metadata": {
        "id": "yDB8fsO2Qg0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "xV_gt7DDPRU0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `prepare_lm_inputs_labels` function is designed to prepare text data for training a language model by creating input-output pairs that reflect how the model will learn to predict the next word in a sequence.\n",
        "\n",
        "**Here’s how it works:**\n",
        "\n",
        "* **Expanding Dimensions:** The function starts by adding an extra dimension to the text data. This adjustment is necessary for compatibility with the vectorization layer, which expects input in a specific shape.\n",
        "\n",
        "* **Tokenization:** Next, the text is converted into numerical indices using the vectorize_layer. This process transforms each word in the text into its corresponding token ID, creating a sequence of integers.\n",
        "\n",
        "* **Creating Input-Output Pairs:** The function then prepares two key components:\n",
        "\n",
        "* **Inputs (x):** It takes all tokens except the last one in each sequence. These tokens serve as the input for the model.\n",
        "* **Labels (y):** It shifts the sequence by one position to the right, so each token in the sequence corresponds to the next word in the original text. These tokens act as the target output the model should learn to predict.\n",
        "\n",
        "In essence, this function arranges the text data into sequences where the model learns to predict the next word based on the words it has seen so far. This setup is fundamental for training a language model to generate coherent and contextually appropriate text."
      ],
      "metadata": {
        "id": "NOvrzYwyQ73T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "k_rMkgj9PTI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}